---
mdate: 2026-02-22 23:12:30
---
1. 多智能体系统 Multi-Agent System, MAS: 由多个具备特定功能的、能够相互通信并协作完成复杂目标的自主实体组成的集合。
2. RLHF Reinforcement Learning from Human Feedback，人类反馈强化学习: 监督微调 (SFT) 训练奖励模型 (Reward Model) 强化学习优化 (PPO/DPO)
3. RLAIF (AI 反馈强化学习) Reinforcement Learning from AI Feedback: 使用 AI 来训练奖励模型
4. PPO (Proximal Policy Optimization): PPO 是强化学习（RL）的传统方案。它的训练过程极其复杂，需要维护 4 个神经网络：
Policy Model (演员)： 正在学习的模型，负责回答问题。
Reference Model (参考)： 保持不动，防止演员学歪了（变得不像人话）。
Reward Model (裁判)： 给回答打分的模型。
Value Model (评论家)： 预测这个回答能得多少分，用来稳定训练。
工作原理： 演员说话 → 裁判打分 → 评论家对比得分与预期 → 算法调整演员的参数。
1. DPO (Direct Preference Optimization):  可以直接通过数学变换，把奖励函数写进损失函数里. 工作原理： 1.  准备好一对对的数据：好数据与坏数据
极简： 只需要 1 个正在练的模型和 1 个参考模型，显存减半。、
稳定： 它不再是强化学习，而变成了分类问题，像跑微调一样稳定。
1. GRPO (Group Relative Policy Optimization): deepseek 训练提出
群组采样 (Group Sampling) 规则打分 (Reward Calculation) 相对优势计算 (Relative Advantage) 