---
mdate: 2026-01-30 17:33:58
---
1. 为什么要搞 FlashAttention？（痛点）
传统的注意力机制（Standard Attention）有一个致命弱点：计算复杂度随序列长度呈“平方级”增长。
内存瓶颈：在计算过程中，它需要生成并存储一个巨大的 N×N 矩阵（Attention Matrix），其中 N 是序列长度。如果你的文档很长，这个矩阵会迅速撑爆 GPU 显存。
读写太慢：GPU 的计算速度极快，但在不同级别的内存（高带宽内存 HBM 和快速 SRAM）之间搬运数据却很慢。传统方法频繁地在这些内存间“读-写-读”，导致显卡空有算力却在等数据搬运。

2. FlashAttention 的“绝招”
它主要通过两个技术手段来解决上述问题：
分块（Tiling）： 它不再一次性计算整个大矩阵，而是把输入拆成一小块一小块（Tiles），放在 GPU 内部运行极快的 SRAM 缓存里计算。算完一块后直接更新结果，不需要把中间那个巨大的 N×N 矩阵存回显存（HBM）。
重计算（Recomputation）： 为了省内存，它在反向传播时不存储中间变量，而是等需要时直接重新算一遍。听起来增加了计算量，但因为省去了缓慢的内存读写时间，实际总速度反而快得多。

