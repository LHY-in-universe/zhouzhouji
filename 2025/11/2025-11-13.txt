---
mdate: 2025-11-13 14:58:12
---
VASP 学习使用
混合专家模型 (MoE)
#为实现大模型的高效训练与推理，研究方向主要有三种：
一是从底层架构入手，如将Transformer架构改为基于状态空间模型（SSM）的Mamba架构；
二是优化预训练微调方法，例如《大模型免微调的上下文对齐方法》中提到的URIAL方法，通过少量示例和系统提示对基础LLM进行对齐；
三是采用混合专家模型（Mixture of Experts，MoE）的大而化之处理方式
# 混合专家模型主要由两个关键部分组成:
稀疏 MoE 层 门控网络或路由